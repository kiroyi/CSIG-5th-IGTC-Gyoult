{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n"
     ]
    }
   ],
   "source": [
    "# 导入包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import zipfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddle.optimizer as optim\n",
    "import paddle.optimizer.lr as lr\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from paddle.io import DataLoader, TensorDataset\n",
    "\n",
    "# 固定随机种子保证结果可复现\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "paddle.seed(seed)\n",
    "\n",
    "# 导入自定义包\n",
    "sys.path.append(\"work\")\n",
    "from candle2 import Canva"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "class MyMSE(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyMSE, self).__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        # 获取 target 的最后一个非 nan 值\n",
    "        target_last = paddle.where(paddle.isnan(target), paddle.to_tensor(float('nan')), target).min(axis=1, keepdim=True)\n",
    "        \n",
    "        # 计算均方误差（MSE）\n",
    "        mse = paddle.mean((input - target_last) ** 2)\n",
    "        \n",
    "        return mse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型-Multichannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedCNN(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super(DilatedCNN, self).__init__()\n",
    "        self.conv = nn.Conv1D(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            padding=(kernel_size - 1) // 2 * dilation, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class BiLSTM(nn.Layer):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            direction='bidirectional', \n",
    "            time_major=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.softmax = nn.Softmax(axis=1)\n",
    "        self.context_vector = paddle.create_parameter(\n",
    "            shape=[hidden_dim * 2], \n",
    "            dtype='float32', \n",
    "            default_initializer=paddle.nn.initializer.Normal()\n",
    "        )\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        attn_weights = nn.ReLU()(self.attn(lstm_out))\n",
    "        attn_weights = paddle.matmul(attn_weights, self.context_vector)\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "        context = paddle.matmul(attn_weights.unsqueeze(1), lstm_out).squeeze(1)\n",
    "        return context\n",
    "\n",
    "class multichannel(nn.Layer):\n",
    "    def __init__(self, input_dim, cnn_out_channels=128, cnn_kernel_size=3, lstm_hidden_dim=256, lstm_num_layers=2, output_dim=1, dropout=0.1):\n",
    "        super(multichannel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim**2*3, 128)\n",
    "        paddle.nn.initializer.KaimingUniform()(self.embedding.weight)\n",
    "        paddle.nn.initializer.Constant(value=0.0)(self.embedding.bias)\n",
    "        self.dilated_cnn1 = DilatedCNN(128, cnn_out_channels, cnn_kernel_size, dilation=1)\n",
    "        self.dilated_cnn2 = DilatedCNN(128, cnn_out_channels, cnn_kernel_size, dilation=2)\n",
    "        self.dilated_cnn3 = DilatedCNN(128, cnn_out_channels, cnn_kernel_size, dilation=3)\n",
    "\n",
    "        self.lstm_cnn = BiLSTM(cnn_out_channels * 3, lstm_hidden_dim, lstm_num_layers)\n",
    "        self.lstm_direct = BiLSTM(128, lstm_hidden_dim, lstm_num_layers)\n",
    "        for layer in [self.lstm_cnn, self.lstm_direct]:\n",
    "            for param in layer.parameters():\n",
    "                if param.ndim >= 2:\n",
    "                    paddle.nn.initializer.XavierUniform()(param)\n",
    "                else:\n",
    "                    paddle.nn.initializer.Constant(value=0.0)(param)\n",
    "                    \n",
    "        self.local_attention = Attention(lstm_hidden_dim)\n",
    "        self.global_attention = Attention(lstm_hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.q = nn.Linear(lstm_hidden_dim * 4, lstm_hidden_dim * 2)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2, output_dim)\n",
    "        paddle.nn.initializer.XavierUniform()(self.q.weight)\n",
    "        paddle.nn.initializer.XavierUniform()(self.fc.weight)\n",
    "        paddle.nn.initializer.Constant(value=0.0)(self.q.bias)\n",
    "        paddle.nn.initializer.Constant(value=0.0)(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - 128.0) / 255.0\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x = paddle.transpose(x, [0, 2, 1]) \n",
    "        # # print(\"Embedding Output Shape:\", x.shape)\n",
    "        # if x.shape[1:] != [128, 128]:\n",
    "        #     pad_shape = [0, 128 - x.shape[2], 0, 128 - x.shape[1]]\n",
    "        #     x = F.pad(x, pad_shape, mode='constant', value=0)  # 扩充到 [batch_size, 128, 128]\n",
    "        # 多通道膨胀卷积\n",
    "        x1 = self.dilated_cnn1(x)\n",
    "        x2 = self.dilated_cnn2(x)\n",
    "        x3 = self.dilated_cnn3(x)\n",
    "        x_cnn = paddle.concat((x1, x2, x3), axis=1)\n",
    "        x_cnn = paddle.transpose(x_cnn, [0, 2, 1])  # (batch_size, seq_len, cnn_out_channels * 3)\n",
    "        # print(\"CNN Output Shape:\", x_cnn.shape)\n",
    "\n",
    "        lstm_cnn_out = self.lstm_cnn(x_cnn)\n",
    "        local_cnn_attn_out = self.local_attention(lstm_cnn_out)\n",
    "\n",
    "        lstm_direct_out = self.lstm_direct(paddle.transpose(x, [0, 2, 1]))\n",
    "        local_direct_attn_out = self.local_attention(lstm_direct_out)\n",
    "\n",
    "\n",
    "        combined_local_attn_out = paddle.concat((local_cnn_attn_out, local_direct_attn_out), axis=1)\n",
    "        \n",
    "        global_attn_out = self.global_attention(paddle.unsqueeze(combined_local_attn_out, axis=1)).squeeze(1)\n",
    "        \n",
    "\n",
    "        output = self.dropout(global_attn_out)\n",
    "        # print(output.shape)\n",
    "        output = self.q(output)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature1~3  对应train_indices1.json\n",
    "# feature4~6  对应2\n",
    "# feature7~8  对应3\n",
    "\n",
    "# 因为受限与显存，这里仅使用0-5w数据作为训练， 5w-7w数据作为测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为 Paddle 张量\n",
    "X_train = np.concatenate([np.load(f\"work/train_dataset/train_features_{i}.npy\") for i in range(1, 2)])[:50000]\n",
    "y_train = np.load(\"work/train_dataset/train_labels1.npy\")[:50000]\n",
    "\n",
    "X_train = paddle.to_tensor(X_train, dtype=paddle.float32).unsqueeze(1)\n",
    "y_train = paddle.to_tensor(y_train, dtype=paddle.float32)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset([X_train, y_train])\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取 500000 条之后的作为验证集\n",
    "X_val = np.concatenate([np.load(f\"work/train_dataset/train_features_{i}.npy\") for i in range(1, 2)])[50000:70000]\n",
    "y_val = np.load(\"work/train_dataset/train_labels3.npy\")[50000:70000]\n",
    "\n",
    "X_val = paddle.to_tensor(X_val, dtype=paddle.float32).unsqueeze(1)\n",
    "y_val = paddle.to_tensor(y_val, dtype=paddle.float32)\n",
    "\n",
    "# 创建数据加载器\n",
    "test_dataset = TensorDataset([X_val, y_val])\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline + 梯度剪裁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型创建\n",
    "model = multichannel(input_dim=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddle.optimizer as optim\n",
    "import paddle.optimizer.lr as lr\n",
    "from copy import deepcopy\n",
    "from paddle.amp import GradScaler, auto_cast\n",
    "import pandas as pd\n",
    "\n",
    "result_dict = {\n",
    "    \"iter\": None,\n",
    "    \"correlation\": None,\n",
    "    \"net\": None,\n",
    "}\n",
    "    \n",
    "# 模型路径\n",
    "save_path = 'work/trained_model/multichannel_gradclip_newdataset.pdparams'\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = MyMSE()\n",
    "\n",
    "# 定义学习率调度器\n",
    "scheduler = lr.StepDecay(learning_rate=5e-5, step_size=50, gamma=0.01)\n",
    "\n",
    "# # 初始化优化器并应用学习率调度器\n",
    "# optimizer = optim.Adam(learning_rate=scheduler, parameters=model.parameters())\n",
    "\n",
    "# 初始化最佳相关系数和最佳模型路径\n",
    "result_dict[\"correlation\"] = -np.inf  # 初始化为负无穷\n",
    "\n",
    "\n",
    "# 定义梯度剪裁策略（例如：根据梯度范数剪裁，设置阈值为1.0）\n",
    "grad_clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)  # 也可以使用 ClipGradByValue(clip_value=0.5) 根据需要调整阈值\n",
    "\n",
    "# 初始化优化器并应用学习率调度器和梯度剪裁\n",
    "optimizer = paddle.optimizer.Adam(\n",
    "    learning_rate=scheduler, \n",
    "    parameters=model.parameters(), \n",
    "    grad_clip=grad_clip  # 应用梯度剪裁\n",
    ")\n",
    "\n",
    "num_epochs = 20  # 可以更改\n",
    "scaler = GradScaler()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.clear_grad()  # 清空梯度\n",
    "\n",
    "        # 检查 NaN 或 Infinity 的输入和目标\n",
    "        if np.any(np.isnan(inputs.numpy())) or np.any(np.isinf(inputs.numpy())):\n",
    "            print(\"Inputs contain NaN or Infinity.\")\n",
    "            continue  # 跳过这个批次\n",
    "        if np.any(np.isnan(targets.numpy())) or np.any(np.isinf(targets.numpy())):\n",
    "            print(\"Targets contain NaN or Infinity.\")\n",
    "            continue  # 跳过这个批次\n",
    "\n",
    "        # 混合精度训练\n",
    "        with paddle.amp.auto_cast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        # 检查 loss 是否为 NaN\n",
    "        if np.isnan(loss.numpy()):\n",
    "            print(\"Loss is NaN. Skipping this batch.\")\n",
    "            continue  # 跳过这个批次\n",
    "\n",
    "        # 使用 GradScaler 缩放损失并反向传播\n",
    "        scaled_loss = scaler.scale(loss)  # 缩放损失\n",
    "        scaled_loss.backward()  # 反向传播\n",
    "        scaler.minimize(optimizer, scaled_loss)  # 优化器更新并取消缩放\n",
    "\n",
    "        optimizer.clear_grad()  # 清除梯度\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}], Loss: {loss.numpy():.4f}')\n",
    "    \n",
    "    # 更新学习率和 scaler\n",
    "    scheduler.step()\n",
    "    scaler.update()\n",
    "\n",
    "    # 每轮训练结束后可以检查或更新 scaler 的缩放值\n",
    "    scaler.update()\n",
    "    \n",
    "    # 验证模型并计算相关系数\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    y_val_pred = []\n",
    "    with paddle.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            y_val_pred.append(outputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "        print(f'Average Test Loss: {total_loss / len(test_loader):.4f}')\n",
    "    \n",
    "    y_val_pred_arr = paddle.concat(y_val_pred, axis=0)\n",
    "    y_val_np = y_val.numpy().reshape(1, -1)\n",
    "    y_val_pred_np = y_val_pred_arr.numpy().reshape(1, -1)\n",
    "\n",
    "    # 这里的 repeat 是为了让 y_val_pred 也能一周有 5 个值，与真实值对应\n",
    "    y_val_pred_np = np.repeat(y_val_pred_np, 5, axis=1)\n",
    "\n",
    "    # 计算预测值与真实值的相关系数\n",
    "    correlation = pd.Series(y_val_pred_np.reshape(-1, )).corr(pd.Series(y_val_np.reshape(-1, )))\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Correlation: {correlation:.4f}')\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if correlation > result_dict[\"correlation\"]:\n",
    "        result_dict[\"iter\"] = epoch\n",
    "        result_dict[\"correlation\"] = correlation\n",
    "        result_dict[\"net\"] = deepcopy(model.state_dict())\n",
    "        \n",
    "paddle.save(result_dict, save_path)\n",
    "print(f'Best model saved with correlation: {result_dict[\"correlation\"]:.4f}, at iter {result_dict[\"iter\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multichannel(input_dim=160)\n",
    "\n",
    "# 加载训练好的模型状态字典。这里指定加载的是最后 1 个 epoch 的模型参数\n",
    "model_state_dict = paddle.load(f'work/trained_model/multichannel_gradclip_newdataset.pdparams')\n",
    "\n",
    "# 将状态字典加载到模型中\n",
    "model.set_state_dict(model_state_dict['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "y_val_pred = []\n",
    "criterion = MyMSE()\n",
    "model.eval()\n",
    "with paddle.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        y_val_pred.append(outputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "    print(f'Average Test Loss: {total_loss / len(test_loader):.4f}')\n",
    "\n",
    "\n",
    "y_val_pred_arr = paddle.concat(y_val_pred, axis=0)\n",
    "y_val_np = y_val.numpy().reshape(1, -1)\n",
    "y_val_pred_np = y_val_pred_arr.numpy().reshape(1, -1)\n",
    "\n",
    "# 这里的 repeat 是为了让 y_val_pred 也能一周有 5 个值，与真实值对应\n",
    "y_val_pred_np = np.repeat(y_val_pred_np, 5, axis=1)\n",
    "\n",
    "# 计算预测值与真实值的相关系数\n",
    "pd.Series(y_val_pred_np.reshape(-1, )).corr(pd.Series(y_val_np.reshape(-1, )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('python35-paddle120-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09f0dbf7b1569c1ab842ae2f41770fe6aa1b54326d081112fa5944b99abb5899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
