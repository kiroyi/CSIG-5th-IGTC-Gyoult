{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:686: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n"
     ]
    }
   ],
   "source": [
    "# 导入包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import zipfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddle.optimizer as optim\n",
    "import paddle.optimizer.lr as lr\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from paddle.io import DataLoader, TensorDataset\n",
    "import json\n",
    "\n",
    "# 固定随机种子保证结果可复现\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "paddle.seed(seed)\n",
    "\n",
    "# 导入自定义包\n",
    "sys.path.append(\"work\")\n",
    "from candle2 import Canva"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型-Multichannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedCNN(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
    "        super(DilatedCNN, self).__init__()\n",
    "        self.conv = nn.Conv1D(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            padding=(kernel_size - 1) // 2 * dilation, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class BiLSTM(nn.Layer):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, \n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            direction='bidirectional', \n",
    "            time_major=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.softmax = nn.Softmax(axis=1)\n",
    "        self.context_vector = paddle.create_parameter(\n",
    "            shape=[hidden_dim * 2], \n",
    "            dtype='float32', \n",
    "            default_initializer=paddle.nn.initializer.Normal()\n",
    "        )\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        attn_weights = nn.ReLU()(self.attn(lstm_out))\n",
    "        attn_weights = paddle.matmul(attn_weights, self.context_vector)\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "        context = paddle.matmul(attn_weights.unsqueeze(1), lstm_out).squeeze(1)\n",
    "        return context\n",
    "\n",
    "class multichannel(nn.Layer):\n",
    "    def __init__(self, input_dim, cnn_out_channels=128, cnn_kernel_size=3, lstm_hidden_dim=256, lstm_num_layers=2, output_dim=1, dropout=0.1):\n",
    "        super(multichannel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim**2*3, 128)\n",
    "        paddle.nn.initializer.KaimingUniform()(self.embedding.weight)\n",
    "        paddle.nn.initializer.Constant(value=0.0)(self.embedding.bias)\n",
    "        self.dilated_cnn1 = DilatedCNN(128, cnn_out_channels, cnn_kernel_size, dilation=1)\n",
    "        self.dilated_cnn2 = DilatedCNN(128, cnn_out_channels, cnn_kernel_size, dilation=2)\n",
    "        self.dilated_cnn3 = DilatedCNN(128, cnn_out_channels, cnn_kernel_size, dilation=3)\n",
    "\n",
    "        self.lstm_cnn = BiLSTM(cnn_out_channels * 3, lstm_hidden_dim, lstm_num_layers)\n",
    "        self.lstm_direct = BiLSTM(128, lstm_hidden_dim, lstm_num_layers)\n",
    "        for layer in [self.lstm_cnn, self.lstm_direct]:\n",
    "            for param in layer.parameters():\n",
    "                if param.ndim >= 2:\n",
    "                    paddle.nn.initializer.XavierUniform()(param)\n",
    "                else:\n",
    "                    paddle.nn.initializer.Constant(value=0.0)(param)\n",
    "                    \n",
    "        self.local_attention = Attention(lstm_hidden_dim)\n",
    "        self.global_attention = Attention(lstm_hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.q = nn.Linear(lstm_hidden_dim * 4, lstm_hidden_dim * 2)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2, output_dim)\n",
    "        paddle.nn.initializer.XavierUniform()(self.q.weight)\n",
    "        paddle.nn.initializer.XavierUniform()(self.fc.weight)\n",
    "        paddle.nn.initializer.Constant(value=0.0)(self.q.bias)\n",
    "        paddle.nn.initializer.Constant(value=0.0)(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - 128.0) / 255.0\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x = paddle.transpose(x, [0, 2, 1]) \n",
    "        # # print(\"Embedding Output Shape:\", x.shape)\n",
    "        # if x.shape[1:] != [128, 128]:\n",
    "        #     pad_shape = [0, 128 - x.shape[2], 0, 128 - x.shape[1]]\n",
    "        #     x = F.pad(x, pad_shape, mode='constant', value=0)  # 扩充到 [batch_size, 128, 128]\n",
    "        # 多通道膨胀卷积\n",
    "        x1 = self.dilated_cnn1(x)\n",
    "        x2 = self.dilated_cnn2(x)\n",
    "        x3 = self.dilated_cnn3(x)\n",
    "        x_cnn = paddle.concat((x1, x2, x3), axis=1)\n",
    "        x_cnn = paddle.transpose(x_cnn, [0, 2, 1])  # (batch_size, seq_len, cnn_out_channels * 3)\n",
    "        # print(\"CNN Output Shape:\", x_cnn.shape)\n",
    "\n",
    "        lstm_cnn_out = self.lstm_cnn(x_cnn)\n",
    "        local_cnn_attn_out = self.local_attention(lstm_cnn_out)\n",
    "\n",
    "        lstm_direct_out = self.lstm_direct(paddle.transpose(x, [0, 2, 1]))\n",
    "        local_direct_attn_out = self.local_attention(lstm_direct_out)\n",
    "\n",
    "\n",
    "        combined_local_attn_out = paddle.concat((local_cnn_attn_out, local_direct_attn_out), axis=1)\n",
    "        \n",
    "        global_attn_out = self.global_attention(paddle.unsqueeze(combined_local_attn_out, axis=1)).squeeze(1)\n",
    "        \n",
    "\n",
    "        output = self.dropout(global_attn_out)\n",
    "        # print(output.shape)\n",
    "        output = self.q(output)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multichannel(input_dim=160)\n",
    "\n",
    "\n",
    "# 加载训练好的模型状态字典\n",
    "model_state_dict = paddle.load(f'work/trained_model/multichannel_gradclip_newdataset.pdparams')\n",
    "\n",
    "# 将状态字典加载到模型中\n",
    "model.set_state_dict(model_state_dict['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = np.stack(all_features, axis=0)\n",
    "# 转换为 Paddle 张量\n",
    "# 共有两个test_features_1.npy 和 test_features_2.npy\n",
    "X_test = np.concatenate([np.load(f\"work/test_dataset/test_features_{i}.npy\") for i in range(1, 2)])\n",
    "# X_test = np.concatenate([np.load(f\"work/test_dataset/test_features_{i}.npy\") for i in range(2, 3)])\n",
    "X_test = paddle.to_tensor(X_test, dtype=paddle.float32).unsqueeze(1)  # Shape becomes [100000, 1, 200, 200]\n",
    "\n",
    "# 因为 TensorDataset 只能接受两个 Tensor，所以需要创建一个占位符才能生成预测结果\n",
    "placeholder = np.zeros([X_test.shape[0], 1]).astype('float32')\n",
    "placeholder = paddle.to_tensor(placeholder, dtype=paddle.float32)\n",
    "\n",
    "# 创建数据集\n",
    "test_dataset = TensorDataset([X_test, placeholder])\n",
    "\n",
    "# 创建数据加载器\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成给预测结果\n",
    "y_pred = []\n",
    "model.eval()\n",
    "\n",
    "with paddle.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        # inputs = inputs[,0]\n",
    "        outputs = model(inputs)\n",
    "        y_pred.append(outputs)\n",
    "\n",
    "y_pred = paddle.concat(y_pred, axis=0).numpy()\n",
    "\n",
    "# # 检查长度是否对的上\n",
    "# assert y_pred.shape[0] == len(okay_indices) + len(modified_indices) + len(misshaped_indices)\n",
    "\n",
    "# 由于预测值代表一周的均值，所以需要复制。如果选手的模型能做到预测每天的 y，那么可以不用 repeat\n",
    "y_pred = np.tile(y_pred, (5, ))\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存张量， 结果过大，需要保存再合并\n",
    "paddle.save(y_pred, \"y_pred1.pdparams\")\n",
    "# paddle.save(y_pred, \"y_pred2.pdparams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取两个 pdparams 文件\n",
    "y_pred1 = paddle.load(\"y_pred1.pdparams\")  # 形状为 (100000, 5)\n",
    "y_pred2 = paddle.load(\"y_pred2.pdparams\")  # 形状为 (39306, 5)\n",
    "\n",
    "# 确保两者的形状兼容（列数相同）\n",
    "assert y_pred1.shape[1] == y_pred2.shape[1], \"列数不匹配，无法连接！\"\n",
    "\n",
    "# 连接两个张量\n",
    "y_pred = paddle.concat([y_pred1, y_pred2], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 JSON 文件\n",
    "with open(\"work/test_dataset/test_indices.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    alls_indices = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取merged_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定文件路径\n",
    "zip_file_path = 'data/data285396/初赛数据集.zip'\n",
    "train_file_name = '数据集/初赛-训练集.csv'\n",
    "test_file_name = '数据集/初赛-测试集.csv'\n",
    "\n",
    "# 打开zip文件\n",
    "with zipfile.ZipFile(zip_file_path) as z:\n",
    "    with z.open(test_file_name) as f:\n",
    "        test_df = pd.read_csv(f, encoding=\"gbk\")\n",
    "\n",
    "\n",
    "# 这里的处理逻辑同训练集，仍然是分组\n",
    "grouper = pd.DataFrame([test_df[\"日期代码\"].unique(), pd.Series((np.diff(test_df[\"日期代码\"].unique()) != 1).cumsum()).shift(1)]).T.bfill().ffill()\n",
    "grouper.columns = ['日期代码', '组别']\n",
    "merged_test = pd.merge(test_df, grouper, on='日期代码', how='left')\n",
    "grouped_test = merged_test.groupby(['股票', '组别'])\n",
    "\n",
    "# 获取每只股票每周对应的日期代码\n",
    "date_dict = dict()\n",
    "\n",
    "for (stock, group), dates in merged_test.groupby(['股票', '组别'])['日期代码']:\n",
    "    date_dict[(stock, group)] = dates.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dates_list = []\n",
    "final_stocks_list = []\n",
    "final_y_pred = []\n",
    "\n",
    "for idx, indices in tqdm(enumerate(alls_indices)):\n",
    "    indices = tuple(indices)\n",
    "    dates = date_dict[indices]\n",
    "    stock = [indices[0]] * len(dates)\n",
    "    score = y_pred[idx][-len(dates):]\n",
    "    final_dates_list.extend(dates)\n",
    "    final_stocks_list.extend(stock)\n",
    "    final_y_pred.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 10000  # 每次合并100个数组\n",
    "final_y_pred_combined = []\n",
    "\n",
    "for i in range(0, len(final_y_pred), batch_size):\n",
    "    batch = final_y_pred[i:i+batch_size]\n",
    "    final_y_pred_combined.append(np.concatenate(batch))\n",
    "\n",
    "# 最终合并所有小批量\n",
    "final_y_pred_combined = np.concatenate(final_y_pred_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(zip(final_stocks_list, final_dates_list, final_y_pred_combined), columns=['股票', '日期代码', 'SCORE'])\n",
    "result_df = result_df.drop_duplicates(subset=['股票', '日期代码'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(test_df, result_df, on=['股票', '日期代码'], how='left')[['股票', '日期代码', 'SCORE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mapping = dict(zip(result_df['日期代码'].unique().tolist(), (result_df['日期代码'].unique()[1:]).tolist() + [40530]))\n",
    "result_df['日期代码'] = result_df['日期代码'].replace(new_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns = ['STOCK', 'NEXT_TRADE_DATE_CODE', 'SCORE']\n",
    "result_df.SCORE.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成提交文件\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "result_df.to_csv(f'work/result/submission_{current_time}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('python35-paddle120-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09f0dbf7b1569c1ab842ae2f41770fe6aa1b54326d081112fa5944b99abb5899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
